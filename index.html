<!DOCTYPE html><html lang="en" data-astro-cid-j7pv25f6> <head><meta charset="utf-8"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="description" content="Yuang Zhang - Researcher at Shanghai Jiao Tong University. Research on video generation, aerial robots, and multi-object tracking."><meta name="generator" content="Astro v5.18.0"><title>Yuang Zhang</title><link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link href="https://fonts.googleapis.com/css2?family=Source+Sans+3:ital,wght@0,300;0,400;0,600;0,700;1,400&display=swap" rel="stylesheet"><link rel="stylesheet" href="/_astro/index.BqTFkSVG.css"></head> <body data-astro-cid-j7pv25f6> <div class="container" data-astro-cid-j7pv25f6> <!-- ===== Bio Section ===== --> <div class="bio-section" data-astro-cid-j7pv25f6> <div class="bio-photo" data-astro-cid-j7pv25f6> <img src="/images/avatars/github-avatar.jpg" alt="Yuang Zhang" data-astro-cid-j7pv25f6> </div> <div class="bio-text" data-astro-cid-j7pv25f6> <h1 data-astro-cid-j7pv25f6>Yuang Zhang</h1> <p data-astro-cid-j7pv25f6>
I am a researcher at <a href="https://www.sjtu.edu.cn/" data-astro-cid-j7pv25f6>Shanghai Jiao Tong University</a>.
			My research focuses on <strong data-astro-cid-j7pv25f6>multi-object tracking</strong>, <strong data-astro-cid-j7pv25f6>video generation</strong>, and <strong data-astro-cid-j7pv25f6>computer vision</strong>.
</p> <p data-astro-cid-j7pv25f6>
I have published at top-tier venues including Nature Machine Intelligence, ICML, CVPR, ECCV, ICCV.
			My representative works include <a href="https://github.com/Tencent/MimicMotion" data-astro-cid-j7pv25f6>MimicMotion</a> for controllable human motion video generation (ICML 2025, â­2.5k on GitHub) and the <a href="https://github.com/megvii-research/MOTR" data-astro-cid-j7pv25f6>MOTR</a> series for end-to-end multi-object tracking (ECCV 2022 &amp; CVPR 2023).
</p> <div class="bio-links" data-astro-cid-j7pv25f6> <a href="https://scholar.google.com/citations?user=pP5WG9wAAAAJ&hl=en" data-astro-cid-j7pv25f6>ğŸ“ Google Scholar</a> <a href="https://github.com/zyayoung" data-astro-cid-j7pv25f6>ğŸ’» GitHub</a> </div> </div> </div> <!-- ===== News ===== --> <h2 class="section-title" data-astro-cid-j7pv25f6>Highlights & Achievements</h2> <ul class="news-list" data-astro-cid-j7pv25f6> <li data-astro-cid-j7pv25f6><span class="news-date" data-astro-cid-j7pv25f6>[2025-05]</span><span data-astro-cid-j7pv25f6>ğŸ† <a href="https://github.com/Tencent/MimicMotion" data-astro-cid-j7pv25f6>MimicMotion</a> accepted to <strong data-astro-cid-j7pv25f6>ICML 2025</strong> â€¢ <span style="color:#f5a623;font-weight:600;" data-astro-cid-j7pv25f6>â­2,539</span> GitHub stars</span></li> <li data-astro-cid-j7pv25f6><span class="news-date" data-astro-cid-j7pv25f6>[2025-01]</span><span data-astro-cid-j7pv25f6>ğŸ¯ <a href="https://henryhuyu.github.io/DiffPhysDrone_Web/" data-astro-cid-j7pv25f6>DiffPhysDrone</a> published in <strong data-astro-cid-j7pv25f6>Nature Machine Intelligence</strong> â€¢ Real world 20m/s autonomous flight!</span></li> <li data-astro-cid-j7pv25f6><span class="news-date" data-astro-cid-j7pv25f6>[2023-03]</span><span data-astro-cid-j7pv25f6>ğŸš€ <a href="https://github.com/megvii-research/MOTRv2" data-astro-cid-j7pv25f6>MOTRv2</a> accepted to <strong data-astro-cid-j7pv25f6>CVPR 2023</strong> â€¢ 1st place in DanceTrack Challenge (73.4% HOTA)</span></li> <li data-astro-cid-j7pv25f6><span class="news-date" data-astro-cid-j7pv25f6>[2022-07]</span><span data-astro-cid-j7pv25f6>ğŸ” <a href="https://github.com/megvii-research/MOTR" data-astro-cid-j7pv25f6>MOTR</a> accepted to <strong data-astro-cid-j7pv25f6>ECCV 2022</strong> â€¢ Pioneer end-to-end tracking with transformer</span></li> <li data-astro-cid-j7pv25f6><span class="news-date" data-astro-cid-j7pv25f6>[2022-05]</span><span data-astro-cid-j7pv25f6>ğŸ‘¥ <a href="https://github.com/zyayoung/Iter-Deformable-DETR" data-astro-cid-j7pv25f6>Iter-DETR</a> accepted to <strong data-astro-cid-j7pv25f6>CVPR 2022</strong> â€¢ Progressive detection in crowded scenes</span></li> <li data-astro-cid-j7pv25f6><span class="news-date" data-astro-cid-j7pv25f6>[Ongoing]</span><span data-astro-cid-j7pv25f6>ğŸ‘¨â€ğŸ’» Open-source projects with 3,800+ total stars â€¢ 1900 citations (Google Scholar)</span></li> </ul> <!-- ===== Selected Publications ===== --> <h2 class="section-title" data-astro-cid-j7pv25f6>Selected Publications</h2> <!-- MimicMotion --> <div class="pub-entry" data-astro-cid-j7pv25f6> <div class="pub-thumb" data-astro-cid-j7pv25f6> <img src="/images/papers/mimicmotion.gif" alt="MimicMotion preview" data-astro-cid-j7pv25f6> <!-- <div class="img-overlay">ğŸ¬ ICML 2025</div> --> </div> <div class="pub-info" data-astro-cid-j7pv25f6> <div class="pub-title" data-astro-cid-j7pv25f6> <a href="https://arxiv.org/abs/2406.19680" data-astro-cid-j7pv25f6>MimicMotion: High-Quality Human Motion Video Generation with Confidence-aware Pose Guidance</a> </div> <div class="pub-authors" data-astro-cid-j7pv25f6> <span class="me" data-astro-cid-j7pv25f6>Yuang Zhang</span>, Jiaxi Gu, Li-Wen Wang, Han Wang, Junqi Cheng, Yuefeng Zhu, Fangyuan Zou
</div> <div class="pub-venue" data-astro-cid-j7pv25f6> <span class="venue-badge spotlight" data-astro-cid-j7pv25f6>ICML 2025</span> <em data-astro-cid-j7pv25f6>International Conference on Machine Learning</em> &nbsp;Â·&nbsp; Cited by 182
</div> <div class="pub-links" data-astro-cid-j7pv25f6> <a href="https://tencent.github.io/MimicMotion/" data-astro-cid-j7pv25f6>ğŸŒ Project Page</a> <a href="https://arxiv.org/abs/2406.19680" data-astro-cid-j7pv25f6>ğŸ“„ Paper</a> <a href="https://github.com/Tencent/MimicMotion" class="star-badge" data-astro-cid-j7pv25f6>â­ 2,539 Stars</a> </div> </div> </div> <!-- ShoulderShot --> <div class="pub-entry" data-astro-cid-j7pv25f6> <div class="pub-thumb" data-astro-cid-j7pv25f6> <div style="display: flex; align-items: center; justify-content: center; width: 100%; height: 100%; background: linear-gradient(135deg, #16a085 0%, #1abc9c 100%); border-radius: 6px; color: white; font-size: 11px; font-weight: 700; text-align: center; padding: 10px; line-height: 1.3; text-shadow: 0 1px 2px rgba(0,0,0,0.2); box-shadow: inset 0 0 20px rgba(26,188,156,0.3);" data-astro-cid-j7pv25f6> <div data-astro-cid-j7pv25f6> <div style="font-size: 13px; margin-bottom: 4px;" data-astro-cid-j7pv25f6>ğŸ¥ ShoulderShot</div> <div style="font-size: 9px; opacity: 0.9;" data-astro-cid-j7pv25f6>Over-the-Shoulder<br data-astro-cid-j7pv25f6>Dialogue Videos</div> </div> </div> </div> <div class="pub-info" data-astro-cid-j7pv25f6> <div class="pub-title" data-astro-cid-j7pv25f6> <a href="https://arxiv.org/abs/2508.07597" data-astro-cid-j7pv25f6>ShoulderShot: Generating Over-the-Shoulder Dialogue Videos</a> </div> <div class="pub-authors" data-astro-cid-j7pv25f6> <span class="me" data-astro-cid-j7pv25f6>Yuang Zhang</span>, Junqi Cheng, Haoyu Zhao, Jiaxi Gu, Fangyuan Zou, Zhen Lu, Pengfei Shu
</div> <div class="pub-venue" data-astro-cid-j7pv25f6> <em data-astro-cid-j7pv25f6>arXiv preprint arXiv:2508.07597</em> </div> <div class="pub-links" data-astro-cid-j7pv25f6> <a href="https://shouldershot.github.io" data-astro-cid-j7pv25f6>ğŸŒ Project Page</a> <a href="https://arxiv.org/abs/2508.07597" data-astro-cid-j7pv25f6>ğŸ“„ Paper</a> </div> </div> </div> <!-- Consistent Video Colorization --> <div class="pub-entry" data-astro-cid-j7pv25f6> <div class="pub-thumb" data-astro-cid-j7pv25f6> <div style="display: flex; align-items: center; justify-content: center; width: 100%; height: 100%; background: linear-gradient(135deg, #e74c3c 0%, #c0392b 100%); border-radius: 6px; color: white; font-size: 11px; font-weight: 700; text-align: center; padding: 10px; line-height: 1.3; text-shadow: 0 1px 2px rgba(0,0,0,0.2); box-shadow: inset 0 0 20px rgba(231,76,60,0.3);" data-astro-cid-j7pv25f6> <div data-astro-cid-j7pv25f6> <div style="font-size: 13px; margin-bottom: 4px;" data-astro-cid-j7pv25f6>ğŸ¨ Video Color</div> <div style="font-size: 9px; opacity: 0.9;" data-astro-cid-j7pv25f6>Palette-Guided<br data-astro-cid-j7pv25f6>Consistency</div> </div> </div> </div> <div class="pub-info" data-astro-cid-j7pv25f6> <div class="pub-title" data-astro-cid-j7pv25f6> <a href="https://arxiv.org/abs/2501.19331" data-astro-cid-j7pv25f6>Consistent Video Colorization via Palette Guidance</a> </div> <div class="pub-authors" data-astro-cid-j7pv25f6>
Han Wang, <span class="me" data-astro-cid-j7pv25f6>Yuang Zhang</span>, Yichen Zhang, Lelin Lu, Liansheng Song
</div> <div class="pub-venue" data-astro-cid-j7pv25f6> <em data-astro-cid-j7pv25f6>arXiv preprint arXiv:2501.19331</em> </div> <div class="pub-links" data-astro-cid-j7pv25f6> <a href="https://arxiv.org/abs/2501.19331" data-astro-cid-j7pv25f6>ğŸ“„ Paper</a> </div> </div> </div> <!-- MOTR --> <div class="pub-entry" data-astro-cid-j7pv25f6> <div class="pub-thumb" data-astro-cid-j7pv25f6> <img src="/images/papers/motr.jpg" alt="MOTR framework" data-astro-cid-j7pv25f6> <!-- <div class="img-overlay">ğŸ” ECCV 2022</div> --> </div> <div class="pub-info" data-astro-cid-j7pv25f6> <div class="pub-title" data-astro-cid-j7pv25f6> <a href="https://arxiv.org/abs/2105.03247" data-astro-cid-j7pv25f6>MOTR: End-to-End Multiple-Object Tracking with Transformer</a> </div> <div class="pub-authors" data-astro-cid-j7pv25f6>
Fangao Zeng, Bin Dong, <span class="me" data-astro-cid-j7pv25f6>Yuang Zhang</span>, Tiancai Wang, Xiangyu Zhang, Yichen Wei
</div> <div class="pub-venue" data-astro-cid-j7pv25f6> <span class="venue-badge" data-astro-cid-j7pv25f6>ECCV 2022</span> <em data-astro-cid-j7pv25f6>European Conference on Computer Vision</em> &nbsp;Â·&nbsp; Cited by 963
</div> <div class="pub-links" data-astro-cid-j7pv25f6> <a href="https://arxiv.org/abs/2105.03247" data-astro-cid-j7pv25f6>ğŸ“„ Paper</a> <a href="https://github.com/megvii-research/MOTR" class="star-badge" data-astro-cid-j7pv25f6>â­ 783 Stars</a> </div> </div> </div> <!-- MOTRv2 --> <div class="pub-entry" data-astro-cid-j7pv25f6> <div class="pub-thumb" data-astro-cid-j7pv25f6> <img src="/images/papers/motrv2-demo.jpg" alt="MOTRv2 demo" data-astro-cid-j7pv25f6> <!-- <div class="img-overlay">ğŸš€ CVPR 2023</div> --> </div> <div class="pub-info" data-astro-cid-j7pv25f6> <div class="pub-title" data-astro-cid-j7pv25f6> <a href="https://arxiv.org/abs/2211.09791" data-astro-cid-j7pv25f6>MOTRv2: Bootstrapping End-to-End Multi-Object Tracking by Pretrained Object Detectors</a> </div> <div class="pub-authors" data-astro-cid-j7pv25f6> <span class="me" data-astro-cid-j7pv25f6>Yuang Zhang</span>, Tiancai Wang, Xiangyu Zhang
</div> <div class="pub-venue" data-astro-cid-j7pv25f6> <span class="venue-badge" data-astro-cid-j7pv25f6>CVPR 2023</span> <em data-astro-cid-j7pv25f6>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> &nbsp;Â·&nbsp; Cited by 324
</div> <div class="pub-links" data-astro-cid-j7pv25f6> <a href="https://arxiv.org/abs/2211.09791" data-astro-cid-j7pv25f6>ğŸ“„ Paper</a> <a href="https://github.com/megvii-research/MOTRv2" class="star-badge" data-astro-cid-j7pv25f6>â­ 472 Stars</a> </div> </div> </div> <!-- DiffPhysDrone --> <div class="pub-entry" data-astro-cid-j7pv25f6> <div class="pub-thumb" data-astro-cid-j7pv25f6> <img src="/images/papers/diffphys-drone.gif" alt="DiffPhysDrone drone" data-astro-cid-j7pv25f6> <!-- <div class="img-overlay">ğŸš Nat. Mach. Intell.</div> --> </div> <div class="pub-info" data-astro-cid-j7pv25f6> <div class="pub-title" data-astro-cid-j7pv25f6> <a href="https://www.nature.com/articles/s42256-025-01048-0" data-astro-cid-j7pv25f6>Learning Vision-Based Agile Flight via Differentiable Physics</a> </div> <div class="pub-authors" data-astro-cid-j7pv25f6> <span class="me" data-astro-cid-j7pv25f6>Yuang Zhang</span>, Yujie Hu, Yuhan Song, Danping Zou, Weijian Lin
</div> <div class="pub-venue" data-astro-cid-j7pv25f6> <span class="venue-badge journal" data-astro-cid-j7pv25f6>Nat. Mach. Intell. 2025</span> <em data-astro-cid-j7pv25f6>Nature Machine Intelligence</em> 7 (6), 954-966 &nbsp;Â·&nbsp; Cited by 53
</div> <div class="pub-links" data-astro-cid-j7pv25f6> <a href="https://henryhuyu.github.io/DiffPhysDrone_Web/" data-astro-cid-j7pv25f6>ğŸŒ Project Page</a> <a href="https://www.nature.com/articles/s42256-025-01048-0" data-astro-cid-j7pv25f6>ğŸ“„ Paper</a> <a href="https://www.bilibili.com/video/BV1sgMqzSExJ/#reply114692001367142" data-astro-cid-j7pv25f6>ğŸ“º Bilibili</a> <a href="https://github.com/HenryHuYu/DiffPhysDrone" class="star-badge" data-astro-cid-j7pv25f6>â­ 467 Stars</a> </div> </div> </div> <!-- Progressive End-to-End Object Detection in Crowded Scenes --> <div class="pub-entry" data-astro-cid-j7pv25f6> <div class="pub-thumb" data-astro-cid-j7pv25f6> <img src="/images/papers/iter-deformable-detr.jpg" alt="Crowded scene detection" data-astro-cid-j7pv25f6> <!-- <div class="img-overlay">ğŸ‘¥ CVPR 2022</div> --> </div> <div class="pub-info" data-astro-cid-j7pv25f6> <div class="pub-title" data-astro-cid-j7pv25f6> <a href="https://arxiv.org/abs/2203.07669" data-astro-cid-j7pv25f6>Progressive End-to-End Object Detection in Crowded Scenes</a> </div> <div class="pub-authors" data-astro-cid-j7pv25f6>
Anlin Zheng, <span class="me" data-astro-cid-j7pv25f6>Yuang Zhang</span>, Xiangyu Zhang, Xiaojuan Qi, Jian Sun
</div> <div class="pub-venue" data-astro-cid-j7pv25f6> <span class="venue-badge" data-astro-cid-j7pv25f6>CVPR 2022</span> <em data-astro-cid-j7pv25f6>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> &nbsp;Â·&nbsp; Cited by 108
</div> <div class="pub-links" data-astro-cid-j7pv25f6> <a href="https://arxiv.org/abs/2203.07669" data-astro-cid-j7pv25f6>ğŸ“„ Paper</a> <a href="https://github.com/zyayoung/Iter-Deformable-DETR" class="star-badge" data-astro-cid-j7pv25f6>â­ 32 Stars</a> </div> </div> </div> <!-- OnlineRefer --> <div class="pub-entry" data-astro-cid-j7pv25f6> <div class="pub-thumb" data-astro-cid-j7pv25f6> <div style="display: flex; align-items: center; justify-content: center; width: 100%; height: 100%; background: linear-gradient(135deg, #ff6b6b 0%, #ee5a24 100%); border-radius: 6px; color: white; font-size: 11px; font-weight: 700; text-align: center; padding: 10px; line-height: 1.3; text-shadow: 0 1px 2px rgba(0,0,0,0.2); box-shadow: inset 0 0 20px rgba(255,255,255,0.1);" data-astro-cid-j7pv25f6> <div data-astro-cid-j7pv25f6> <div style="font-size: 13px; margin-bottom: 4px;" data-astro-cid-j7pv25f6>ğŸ¯ OnlineRefer</div> <div style="font-size: 9px; opacity: 0.9;" data-astro-cid-j7pv25f6>Real-time Video<br data-astro-cid-j7pv25f6>Object Segmentation</div> </div> </div> </div> <div class="pub-info" data-astro-cid-j7pv25f6> <div class="pub-title" data-astro-cid-j7pv25f6> <a href="https://arxiv.org/abs/2307.09356" data-astro-cid-j7pv25f6>OnlineRefer: A Simple Online Baseline for Referring Video Object Segmentation</a> </div> <div class="pub-authors" data-astro-cid-j7pv25f6>
Dongming Wu, Tiancai Wang, <span class="me" data-astro-cid-j7pv25f6>Yuang Zhang</span>, Xiangyu Zhang, Jianbing Shen
</div> <div class="pub-venue" data-astro-cid-j7pv25f6> <span class="venue-badge" data-astro-cid-j7pv25f6>ICCV 2023</span> <em data-astro-cid-j7pv25f6>IEEE/CVF International Conference on Computer Vision</em> &nbsp;Â·&nbsp; Cited by 103
</div> <div class="pub-links" data-astro-cid-j7pv25f6> <a href="https://arxiv.org/abs/2308.07661" data-astro-cid-j7pv25f6>ğŸ“„ Paper</a> <a href="https://github.com/wudongming97/OnlineRefer" class="star-badge" data-astro-cid-j7pv25f6>â­ GitHub</a> </div> </div> </div> <!-- MOTRv3 --> <div class="pub-entry" data-astro-cid-j7pv25f6> <div class="pub-thumb" data-astro-cid-j7pv25f6> <div style="display: flex; align-items: center; justify-content: center; width: 100%; height: 100%; background: linear-gradient(135deg, #2c3e50 0%, #3498db 100%); border-radius: 6px; color: white; font-size: 11px; font-weight: 700; text-align: center; padding: 10px; line-height: 1.3; text-shadow: 0 1px 2px rgba(0,0,0,0.3); box-shadow: inset 0 0 25px rgba(52,152,219,0.3);" data-astro-cid-j7pv25f6> <div data-astro-cid-j7pv25f6> <div style="font-size: 13px; margin-bottom: 4px;" data-astro-cid-j7pv25f6>ğŸš€ MOTRv3</div> <div style="font-size: 9px; opacity: 0.9;" data-astro-cid-j7pv25f6>Release-Fetch<br data-astro-cid-j7pv25f6>Supervision</div> </div> </div> </div> <div class="pub-info" data-astro-cid-j7pv25f6> <div class="pub-title" data-astro-cid-j7pv25f6> <a href="https://arxiv.org/abs/2305.14298" data-astro-cid-j7pv25f6>MOTRv3: Release-Fetch Supervision for End-to-End Multi-Object Tracking</a> </div> <div class="pub-authors" data-astro-cid-j7pv25f6>
En Yu, Tiancai Wang, Zhuoling Li, <span class="me" data-astro-cid-j7pv25f6>Yuang Zhang</span>, Xiangyu Zhang, Wenbing Tao
</div> <div class="pub-venue" data-astro-cid-j7pv25f6> <em data-astro-cid-j7pv25f6>arXiv preprint arXiv:2305.14298</em> &nbsp;Â·&nbsp; Cited by 69
</div> <div class="pub-links" data-astro-cid-j7pv25f6> <a href="https://arxiv.org/abs/2305.14298" data-astro-cid-j7pv25f6>ğŸ“„ Paper</a> </div> </div> </div> <!-- Variational Pedestrian Detection --> <div class="pub-entry" data-astro-cid-j7pv25f6> <div class="pub-thumb" data-astro-cid-j7pv25f6> <div style="display: flex; align-items: center; justify-content: center; width: 100%; height: 100%; background: linear-gradient(135deg, #27ae60 0%, #2ecc71 100%); border-radius: 6px; color: white; font-size: 11px; font-weight: 700; text-align: center; padding: 10px; line-height: 1.3; text-shadow: 0 1px 2px rgba(0,0,0,0.2); box-shadow: inset 0 0 20px rgba(46,204,113,0.3);" data-astro-cid-j7pv25f6> <div data-astro-cid-j7pv25f6> <div style="font-size: 13px; margin-bottom: 4px;" data-astro-cid-j7pv25f6>ğŸš¶ Variational</div> <div style="font-size: 9px; opacity: 0.9;" data-astro-cid-j7pv25f6>Pedestrian<br data-astro-cid-j7pv25f6>Detection</div> </div> </div> </div> <div class="pub-info" data-astro-cid-j7pv25f6> <div class="pub-title" data-astro-cid-j7pv25f6> <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Variational_Pedestrian_Detection_CVPR_2021_paper.html" data-astro-cid-j7pv25f6>Variational Pedestrian Detection</a> </div> <div class="pub-authors" data-astro-cid-j7pv25f6> <span class="me" data-astro-cid-j7pv25f6>Yuang Zhang</span>, He He, Jianguo Li, Yumin Li, John See, Weiyao Lin
</div> <div class="pub-venue" data-astro-cid-j7pv25f6> <span class="venue-badge" data-astro-cid-j7pv25f6>CVPR 2021</span> <em data-astro-cid-j7pv25f6>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> &nbsp;Â·&nbsp; Cited by 56
</div> <div class="pub-links" data-astro-cid-j7pv25f6> <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Variational_Pedestrian_Detection_CVPR_2021_paper.html" data-astro-cid-j7pv25f6>ğŸ“„ Paper</a> </div> </div> </div> <!-- Seeing through pixel motion --> <div class="pub-entry" data-astro-cid-j7pv25f6> <div class="pub-thumb" data-astro-cid-j7pv25f6> <div style="display: flex; align-items: center; justify-content: center; width: 100%; height: 100%; background: linear-gradient(135deg, #8e44ad 0%, #9b59b6 100%); border-radius: 6px; color: white; font-size: 11px; font-weight: 700; text-align: center; padding: 10px; line-height: 1.3; text-shadow: 0 1px 2px rgba(0,0,0,0.2); box-shadow: inset 0 0 20px rgba(155,89,182,0.3);" data-astro-cid-j7pv25f6> <div data-astro-cid-j7pv25f6> <div style="font-size: 13px; margin-bottom: 4px;" data-astro-cid-j7pv25f6>ğŸ‘ï¸ Pixel Motion</div> <div style="font-size: 9px; opacity: 0.9;" data-astro-cid-j7pv25f6>Optical Flow<br data-astro-cid-j7pv25f6>Obstacle Avoidance</div> </div> </div> </div> <div class="pub-info" data-astro-cid-j7pv25f6> <div class="pub-title" data-astro-cid-j7pv25f6> <a href="https://arxiv.org/abs/2411.04413" data-astro-cid-j7pv25f6>Seeing Through Pixel Motion: Learning Obstacle Avoidance from Optical Flow with One Camera</a> </div> <div class="pub-authors" data-astro-cid-j7pv25f6>
Yujie Hu, <span class="me" data-astro-cid-j7pv25f6>Yuang Zhang</span>, Yuhan Song, Yilin Deng, Fei Yu, Lele Zhang, Weijian Lin, Danping Zou, Wei Yu
</div> <div class="pub-venue" data-astro-cid-j7pv25f6> <span class="venue-badge journal" data-astro-cid-j7pv25f6>RA-L 2025</span> <em data-astro-cid-j7pv25f6>IEEE Robotics and Automation Letters</em> &nbsp;Â·&nbsp; Cited by 19
</div> <div class="pub-links" data-astro-cid-j7pv25f6> <a href="https://arxiv.org/abs/2409.09908" data-astro-cid-j7pv25f6>ğŸ“„ Paper</a> </div> </div> </div> <!-- VLM-Eval --> <div class="pub-entry" data-astro-cid-j7pv25f6> <div class="pub-thumb" data-astro-cid-j7pv25f6> <div style="display: flex; align-items: center; justify-content: center; width: 100%; height: 100%; background: linear-gradient(135deg, #e67e22 0%, #f39c12 100%); border-radius: 6px; color: white; font-size: 11px; font-weight: 700; text-align: center; padding: 10px; line-height: 1.3; text-shadow: 0 1px 2px rgba(0,0,0,0.2); box-shadow: inset 0 0 20px rgba(243,156,18,0.3);" data-astro-cid-j7pv25f6> <div data-astro-cid-j7pv25f6> <div style="font-size: 13px; margin-bottom: 4px;" data-astro-cid-j7pv25f6>ğŸ“Š VLM-Eval</div> <div style="font-size: 9px; opacity: 0.9;" data-astro-cid-j7pv25f6>Video LLM<br data-astro-cid-j7pv25f6>Evaluation</div> </div> </div> </div> <div class="pub-info" data-astro-cid-j7pv25f6> <div class="pub-title" data-astro-cid-j7pv25f6> <a href="https://arxiv.org/abs/2311.11865" data-astro-cid-j7pv25f6>VLM-Eval: A General Evaluation on Video Large Language Models</a> </div> <div class="pub-authors" data-astro-cid-j7pv25f6>
Shuailin Li, <span class="me" data-astro-cid-j7pv25f6>Yuang Zhang</span>, Yucheng Zhao, Qiuyue Wang, Fan Jia, Yingfei Liu, Tiancai Wang
</div> <div class="pub-venue" data-astro-cid-j7pv25f6> <em data-astro-cid-j7pv25f6>arXiv preprint arXiv:2311.11865</em> &nbsp;Â·&nbsp; Cited by 12
</div> <div class="pub-links" data-astro-cid-j7pv25f6> <a href="https://arxiv.org/abs/2311.11865" data-astro-cid-j7pv25f6>ğŸ“„ Paper</a> <a href="https://github.com/zyayoung/Awesome-Video-LLMs" class="star-badge" data-astro-cid-j7pv25f6>â­ 37 Stars</a> </div> </div> </div> <!-- ===== All Publications link ===== --> <p style="text-align: center; margin-top: 24px; font-size: 15px; color: #888;" data-astro-cid-j7pv25f6>
For a full list of publications, please visit my
<a href="https://scholar.google.com/citations?user=pP5WG9wAAAAJ&hl=en" data-astro-cid-j7pv25f6>Google Scholar</a> profile.
</p> <!-- ===== Open Source ===== --> <h2 class="section-title" data-astro-cid-j7pv25f6>Open Source Projects</h2> <p style="color:#666; margin-bottom: 20px; font-size: 15px;" data-astro-cid-j7pv25f6>Maintain 6+ open-source repositories with <span style="font-weight:600;" data-astro-cid-j7pv25f6>3,800+ stars</span> across projects. Featured in top-tier conferences.</p> <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(260px, 1fr)); gap: 18px;" data-astro-cid-j7pv25f6> <a href="https://github.com/Tencent/MimicMotion" style="display: block; padding: 20px; border: 1px solid #eee; border-radius: 10px; transition: all 0.2s; background: linear-gradient(135deg, #fff 0%, #fff8f8 100%); text-decoration: none;" onmouseover="this.style.transform='translateY(-4px)'; this.style.boxShadow='0 6px 16px rgba(245,166,35,0.15)'; this.style.borderColor='#ffd6cc'" onmouseout="this.style.transform='none'; this.style.boxShadow='none'; this.style.borderColor='#eee'" data-astro-cid-j7pv25f6> <div style="display: flex; align-items: center; gap: 10px; margin-bottom: 10px;" data-astro-cid-j7pv25f6> <img src="/images/papers/mimicmotion.gif" alt="" style="width: 32px; height: 32px;" data-astro-cid-j7pv25f6> <div data-astro-cid-j7pv25f6> <div style="font-weight: 700; font-size: 16px; color: #222;" data-astro-cid-j7pv25f6>ğŸ¬ MimicMotion</div> <div style="font-size: 13px; color: #888;" data-astro-cid-j7pv25f6>ICML 2025 (Spotlight)</div> </div> </div> <div style="font-size: 14px; color: #555; margin-bottom: 14px;" data-astro-cid-j7pv25f6>High-quality human motion video generation with confidence-aware pose guidance</div> <div style="display: flex; justify-content: space-between; align-items: center;" data-astro-cid-j7pv25f6> <div style="font-size: 14px; color: #f5a623; font-weight: 700;" data-astro-cid-j7pv25f6>â­ 2,539</div> <div style="font-size: 13px; color: #777;" data-astro-cid-j7pv25f6>ğŸ¯ Project: <span style="color:#1772d0;" data-astro-cid-j7pv25f6>tencent.github.io/MimicMotion</span></div> </div> </a> <a href="https://github.com/megvii-research/MOTR" style="display: block; padding: 20px; border: 1px solid #eee; border-radius: 10px; transition: all 0.2s; background: linear-gradient(135deg, #fff 0%, #f8faff 100%); text-decoration: none;" onmouseover="this.style.transform='translateY(-4px)'; this.style.boxShadow='0 6px 16px rgba(23,114,208,0.15)'; this.style.borderColor='#ccdfff'" onmouseout="this.style.transform='none'; this.style.boxShadow='none'; this.style.borderColor='#eee'" data-astro-cid-j7pv25f6> <div style="display: flex; align-items: center; gap: 10px; margin-bottom: 10px;" data-astro-cid-j7pv25f6> <img src="/images/papers/motr.jpg" alt="" style="width: 32px; height: 32px; object-fit: cover; border-radius: 4px;" data-astro-cid-j7pv25f6> <div data-astro-cid-j7pv25f6> <div style="font-weight: 700; font-size: 16px; color: #222;" data-astro-cid-j7pv25f6>ğŸ” MOTR</div> <div style="font-size: 13px; color: #888;" data-astro-cid-j7pv25f6>ECCV 2022</div> </div> </div> <div style="font-size: 14px; color: #555; margin-bottom: 14px;" data-astro-cid-j7pv25f6>End-to-end multiple-object tracking with transformer â€¢ Cited by 963</div> <div style="display: flex; justify-content: space-between; align-items: center;" data-astro-cid-j7pv25f6> <div style="font-size: 14px; color: #f5a623; font-weight: 700;" data-astro-cid-j7pv25f6>â­ 783</div> <div style="font-size: 13px; color: #777;" data-astro-cid-j7pv25f6>ğŸ“Š Framework: <span style="color:#1772d0;" data-astro-cid-j7pv25f6>figs/motr.png</span></div> </div> </a> <a href="https://github.com/megvii-research/MOTRv2" style="display: block; padding: 20px; border: 1px solid #eee; border-radius: 10px; transition: all 0.2s; background: linear-gradient(135deg, #fff 0%, #f8fff8 100%); text-decoration: none;" onmouseover="this.style.transform='translateY(-4px)'; this.style.boxShadow='0 6px 16px rgba(67,233,123,0.15)'; this.style.borderColor='#d0ffd6'" onmouseout="this.style.transform='none'; this.style.boxShadow='none'; this.style.borderColor='#eee'" data-astro-cid-j7pv25f6> <div style="display: flex; align-items: center; gap: 10px; margin-bottom: 10px;" data-astro-cid-j7pv25f6> <img src="https://raw.githubusercontent.com/zyayoung/oss/main/motrv2_main.jpg" alt="" style="width: 32px; height: 32px; object-fit: cover; border-radius: 4px;" data-astro-cid-j7pv25f6> <div data-astro-cid-j7pv25f6> <div style="font-weight: 700; font-size: 16px; color: #222;" data-astro-cid-j7pv25f6>ğŸš€ MOTRv2</div> <div style="font-size: 13px; color: #888;" data-astro-cid-j7pv25f6>CVPR 2023</div> </div> </div> <div style="font-size: 14px; color: #555; margin-bottom: 14px;" data-astro-cid-j7pv25f6>1st place in DanceTrack Challenge (73.4% HOTA) â€¢ 324 citations</div> <div style="display: flex; justify-content: space-between; align-items: center;" data-astro-cid-j7pv25f6> <div style="font-size: 14px; color: #f5a623; font-weight: 700;" data-astro-cid-j7pv25f6>â­ 472</div> <div style="font-size: 13px; color: #777;" data-astro-cid-j7pv25f6>ğŸ¬ Demo: <span style="color:#1772d0;" data-astro-cid-j7pv25f6>2_motrv2.gif</span></div> </div> </a> <a href="https://github.com/HenryHuYu/DiffPhysDrone" style="display: block; padding: 20px; border: 1px solid #eee; border-radius: 10px; transition: all 0.2s; background: linear-gradient(135deg, #fff 0%, #fff5f5 100%); text-decoration: none;" onmouseover="this.style.transform='translateY(-4px)'; this.style.boxShadow='0 6px 16px rgba(250,112,154,0.15)'; this.style.borderColor='#ffd6e0'" onmouseout="this.style.transform='none'; this.style.boxShadow='none'; this.style.borderColor='#eee'" data-astro-cid-j7pv25f6> <div style="display: flex; align-items: center; gap: 10px; margin-bottom: 10px;" data-astro-cid-j7pv25f6> <img src="https://github.com/HenryHuYu/DiffPhysDrone/blob/master/gifs/20ms.gif?raw=true" alt="" style="width: 32px; height: 32px; object-fit: cover; border-radius: 4px;" data-astro-cid-j7pv25f6> <div data-astro-cid-j7pv25f6> <div style="font-weight: 700; font-size: 16px; color: #222;" data-astro-cid-j7pv25f6>ğŸš DiffPhysDrone</div> <div style="font-size: 13px; color: #888;" data-astro-cid-j7pv25f6>Nature Machine Intelligence</div> </div> </div> <div style="font-size: 14px; color: #555; margin-bottom: 14px;" data-astro-cid-j7pv25f6>First real quadrotor trained via differentiable physics â€¢ 53 citations</div> <div style="display: flex; justify-content: space-between; align-items: center;" data-astro-cid-j7pv25f6> <div style="font-size: 14px; color: #f5a623; font-weight: 700;" data-astro-cid-j7pv25f6>â­ 467</div> <div style="font-size: 13px; color: #777;" data-astro-cid-j7pv25f6>ğŸŒ Project: <span style="color:#1772d0;" data-astro-cid-j7pv25f6>henryhuyu.github.io/DiffPhysDrone_Web</span></div> </div> </a> <a href="https://github.com/zyayoung" style="display: block; padding: 20px; border: 1px solid #eee; border-radius: 10px; transition: all 0.2s; background: linear-gradient(135deg, #fff 0%, #f5f8ff 100%); text-decoration: none;" onmouseover="this.style.transform='translateY(-4px)'; this.style.boxShadow='0 6px 16px rgba(161,196,253,0.15)'; this.style.borderColor='#ccdfff'" onmouseout="this.style.transform='none'; this.style.boxShadow='none'; this.style.borderColor='#eee'" data-astro-cid-j7pv25f6> <div style="display: flex; align-items: center; gap: 10px; margin-bottom: 10px;" data-astro-cid-j7pv25f6> <img src="https://avatars.githubusercontent.com/u/6887683?v=4" alt="" style="width: 32px; height: 32px; border-radius: 50%;" data-astro-cid-j7pv25f6> <div data-astro-cid-j7pv25f6> <div style="font-weight: 700; font-size: 16px; color: #222;" data-astro-cid-j7pv25f6>ğŸ’» All Projects</div> <div style="font-size: 13px; color: #888;" data-astro-cid-j7pv25f6>GitHub Profile</div> </div> </div> <div style="font-size: 14px; color: #555; margin-bottom: 14px;" data-astro-cid-j7pv25f6>View all open-source contributions, including variational detection, online video segmentation, and more</div> <div style="display: flex; justify-content: space-between; align-items: center;" data-astro-cid-j7pv25f6> <div style="font-size: 14px; color: #1772d0; font-weight: 700;" data-astro-cid-j7pv25f6>ğŸ“‚ 10+ Repositories</div> <div style="font-size: 13px; color: #777;" data-astro-cid-j7pv25f6>â­ 3,800+ total stars</div> </div> </a> </div> <!-- ===== Footer ===== --> <footer data-astro-cid-j7pv25f6> <p data-astro-cid-j7pv25f6>Â© 2026 Yuang Zhang Â· Built with <a href="https://astro.build/" data-astro-cid-j7pv25f6>Astro</a> Â· Hosted on <a href="https://pages.github.com/" data-astro-cid-j7pv25f6>GitHub Pages</a></p> <p style="margin-top: 4px;" data-astro-cid-j7pv25f6>Design inspired by <a href="https://jonbarron.info/" data-astro-cid-j7pv25f6>Jon Barron</a></p> </footer> </div> </body></html>